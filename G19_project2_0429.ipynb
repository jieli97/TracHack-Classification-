{"cells":[{"cell_type":"code","source":["df = spark.read.parquet('dbfs:/mnt/bigdataproject649/parquet5/')\ndf_sample = df.sample(withReplacement=False,fraction=.1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["num_files = 1\ndf_sample_path = \"/FileStore/tables/temp5\"\n#df_sample.coalesce(num_files).write.option(\"mode\", \"overwrite\").option(\"compression\", \"bzip2\").json(df_sample_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import datediff\nfrom pyspark.sql.functions import last_day \nfrom pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["def select_data(data_frame):\n    \"\"\"Selects and transforms the raw GCR (JSON) records data frame into a data frame.\n\n    :param data_frame: Input data frame to select the specific data elements from.\n    :returns selected dataframe that includes the columns of interest.\n    \"\"\"\n    #process \"network-usage-domestic\" to a individual column\n    df_net_usage = df_sample.select(func.col(\"customer_info.line_id\").alias('line_id'),\n                               func.explode(\"network-usage-domestic\").alias('net_usage_by_day'))\n    df_net_usage_table = df_net_usage.select('line_id', 'net_usage_by_day.*')\n    agg_df_net_usage_table=df_net_usage_table.groupBy('line_id').avg()\n    df_net_usage_daily=agg_df_net_usage_table.select('line_id','avg(data)','avg(mms_out)','avg(sms_out)',\n                                                   'avg(voice_duration_out)','avg(voice_num_total)')\n\n    df_net_usage_daily = df_net_usage_daily.withColumnRenamed(\"avg(data)\",\"daily_data_volume\")\n    df_net_usage_daily = df_net_usage_daily.withColumnRenamed(\"avg(mms_out)\",\"daily_mms_volume\")\n    df_net_usage_daily = df_net_usage_daily.withColumnRenamed(\"avg(sms_out)\",\"daily_sms_volume\")\n    df_net_usage_daily = df_net_usage_daily.withColumnRenamed(\"avg(voice_duration_out)\",\"daily_voice_time\")\n    df_net_usage_daily = df_net_usage_daily.withColumnRenamed(\"avg(voice_num_total)\",\"daily_voice_volume\")\n    df_phone_info = df_sample.select(func.col(\"customer_info.line_id\").alias('line_id'),  \n                                     func.col(\"phone_info.*\"))\n    df_phone_info = df_phone_info.fillna({'manufacturer': \"unknown\", 'retailer': \"unknown\", \"model\": \"unknown\"})\n    df_reactive_info = df_sample.select(func.col(\"customer_info.line_id\").alias('line_id'),  \n                                   func.explode(\"reactivations\").alias('reactive_status')                                        \n                                     )\n    df_reactive_info = df_reactive_info.select('line_id', 'reactive_status.*')\n    #calculate reactive times per person \n    df_reactive_info_count=df_reactive_info.groupBy(\"line_id\").count()\n    df_reactive_info= df_reactive_info_count.withColumnRenamed(\"count\",\"reactive_times\")\n    # explore reactivation per record\n    df_deactive_info = df_sample.select(func.col(\"customer_info.line_id\").alias('line_id'),  \n                                   func.explode(\"deactivations\").alias('deactive_status')                                        \n                                     )\n    df_deactive_info = df_deactive_info.select('line_id', 'deactive_status.*')\n    #calculate reactive times per person \n    grouped=df_deactive_info.groupBy('line_id')\n    df_maxddate= grouped.agg({'deactivation_date':'max'}).withColumnRenamed(\"max(deactivation_date)\",\"latest_ddate\")\n    df_maxddate= df_maxddate.filter( df_maxddate[\"latest_ddate\"] < '2020-03-19')\n    \n    df_redemption = df_sample.select(func.col(\"customer_info.line_id\").alias('line_id'),\n                                 func.explode(\"redemptions\").alias('redemptions_by_day'))\n    df_redemption_table = df_redemption.select('line_id', 'redemptions_by_day.*')\n    # calculate gross revenue per person\n    df_grossrev_redemption = df_redemption_table.select('line_id', \n                                                    func.col('gross_revenue').cast(IntegerType()))\n    df_grossrev_redemption_table = df_grossrev_redemption.groupby('line_id').sum('gross_revenue')\n    df_grossrev_redemption_table = df_grossrev_redemption_table.withColumnRenamed(\"sum(gross_revenue)\",'gross_redemption_revenue')\n    df_cc = df_sample.select(func.col(\"customer_info.line_id\").alias(\"line_id\"),\n                         func.col(\"contact_info.city\").alias(\"city\"),\n                         func.col(\"contact_info.country\").alias(\"country\"),\n                         func.col(\"contact_info.state\").alias(\"state\"),\n                         func.col(\"contact_info.zipcode\").alias(\"zipcode\"),  \n                         func.col(\"status\"),\n                        func.col(\"customer_info.birth_year\").alias(\"birth_year\").cast(IntegerType()),\n                        func.col(\"customer_info.first_activation_date\").alias(\"first_act\"),\n                         func.col(\"customer_info.lifetime_redemptions\").alias(\"redemption\"),\n                        func.col(\"customer_info.lifetime_revenues\").alias(\"revenue\")\n                       )\n    def funct(birth):\n      if birth == 1753:\n        return 1976\n      else:\n        return birth\n\n    func_udf = func.udf(funct, IntegerType())\n    df_cc = df_cc.withColumn(\"birth_year\", func_udf(df_cc['birth_year']))\n    df_cc = df_cc.fillna({\"birth_year\": 1976})\n    df_cc = df_cc.filter( df_cc[\"first_act\"] < '2020-03-19')\n    df1 = df_cc.join(df_phone_info, on=['line_id'], how='left')\n    df2 = df1.join(df_reactive_info, on=['line_id'], how='left')\n    df3 = df2.join(df_grossrev_redemption_table, on=['line_id'], how='left')\n    df4=df3.join(df_net_usage_daily, on=['line_id'], how='left')\n    df5=df4.join(df_maxddate, on=['line_id'], how='left')\n    df_final=df5.select('line_id','status','city','state','birth_year','redemption',\n                      'revenue','device_type','manufacturer','operating_system','volte','reactive_times',\n                        'gross_redemption_revenue','first_act','latest_ddate','daily_data_volume',\n                      'daily_mms_volume','daily_sms_volume','daily_voice_time','daily_voice_volume')\n    df_final = df_final.withColumn('diff', datediff(df_final.latest_ddate,df_final.first_act))\n    df_final = df_final.fillna( { \"volte\": 'N', \"reactive_times\": 0, \n                                 \"latest_ddate\": '2020-03-19','city':'None',\n                                 'state':'None','device_type':'None','manufacturer':'None',\n                                 'operating_system':'None',\n                                 'redemption':0,'revenue':0,'reactive_times':0,\n                                 'gross_redemption_revenue':0,'diff':0,'daily_data_volume':0,\n                      'daily_mms_volume':0,'daily_sms_volume': 0,'daily_voice_time':0,'daily_voice_volume':0} )\n    return df_final"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def featurize_data(df, remove_orig_cols=True):\n    \"\"\"Given a selected data frame, generate a featurized dataframe.\n\n    Example: Shows how categorical features are handled by first a string indexer and then one-hot encoding.\n\n    if remove_orig_cols is set, only returns a dataframe with two columns - features and label.\n\n\n    :param df: Input data frame to featurize.\n    :param remove_orig_cols: (Default=True) If set we remove the original columns from the returned dataframe.\n    :returns featurized dataframe that includes two columns - label and features. If remove_orig_cols=False, includes\n    the columns from original input data frame as well.\n    \"\"\"\n\n    stages = []\n\n    categorical_columns = ['city','state','device_type','manufacturer','operating_system','volte']\n    for column in categorical_columns:\n        string_indexer = StringIndexer(inputCol=column, outputCol=column + '_index')\n        encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"_vec\"])\n        stages += [string_indexer, encoder]\n\n    label_indexer = StringIndexer(inputCol=\"status\", outputCol=\"label\")\n    stages += [label_indexer]\n\n    numeric_columns = ['birth_year','redemption','revenue','reactive_times','gross_redemption_revenue','diff','daily_data_volume',\n                      'daily_mms_volume','daily_sms_volume','daily_voice_time','daily_voice_volume']\n\n    assembler_inputs = [c + \"_vec\" for c in categorical_columns] + numeric_columns\n    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n    stages += [assembler]\n\n    pipeline = Pipeline(stages=stages)\n    pipeline_model = pipeline.fit(df)\n    df_featurized = pipeline_model.transform(df)\n    if remove_orig_cols:\n        selected_columns = ['features', 'label']\n        return df_featurized.select(selected_columns)\n    else:\n        return df_featurized\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def train_model(train_dataset):\n    \"\"\"Given a featurized training dataset, trains a simple logistic regression model and\n    returns the trained model object.\n\n    :param train_dataset: A dataframe with two columns - label and features.\n    :returns trained model object.\n    \"\"\"\n    rfc = RandomForestClassifier(numTrees=150, featuresCol='features', labelCol='label')\n    rfc_model = rfc.fit(train_dataset)\n    return rfc_model\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def evaluate_model(model, test_dataset):\n    \"\"\"Given a model and featurized test dataset, returns the auc value.\n\n    :param model: the pyspark.ml model object to evaluate.\n    :param test_dataset: A dataframe with two columns - label and features, to evaluate the model.\n    :returns AUC under ROC value.\n\n    \"\"\"\n    predictions = model.transform(test_dataset)\n    evaluator = BinaryClassificationEvaluator()\n    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n    return auc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["def trachack_submission(spark, data_path, random_seed, test_ratio=0.2, num_folds=10):\n    \"\"\"The end to end model pipeline, printing out the AUC value for each fold and the averaged AUC value.\n\n    :param spark: SparkSession object.\n    :param data_path: path to the dataset to use for the pipeline.\n    :param random_seed: seed as input to the seed selection for randomSplit for train/test split.\n    :param test_ratio: (Default=0.2) Percentage of data to use for test/evaluation. Must be between 0 and 1.\n    :param num_folds: (Default=10) Number of folds for averaged AUC value.\n    :returns: averaged AUC value\n    \"\"\"\n    df = spark.read.json(data_path)\n    df_selected = select_data(df)\n    df_featurized = featurize_data(df_selected)\n\n    # Cache this data frame since we will be doing multiple passes to split, train and evaluate\n    df_featurized.cache()\n\n    fold_auc = []\n    for i in range(num_folds):\n        fold_seed = random_seed * i\n        train, test = df_featurized.randomSplit([1.0 - test_ratio, test_ratio], seed=fold_seed)\n        model = train_model(train)\n        auc = evaluate_model(model, test)\n        print(f\"Fold {i} AUC: {auc}\")\n        fold_auc.append(auc)\n\n    average_auc = sum(fold_auc) / num_folds\n    print(\"Average AUC: {average_auc}\")\n    return average_auc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["trachack_submission(spark, df_sample_path, 123)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"project2 draft","notebookId":3865364096458658},"nbformat":4,"nbformat_minor":0}
